"use strict";(self.webpackChunklhtp=self.webpackChunklhtp||[]).push([[39453],{28453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>r});var s=n(96540);const a={},i=s.createContext(a);function o(e){const t=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:t},e.children)}},70137:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"intermediate-javascript/test-driven-development-and-environments-with-javascript/2-2-1-8-testing-best-practices","title":"\ud83d\udcd3 2.2.1.8 Testing Best Practices","description":"In this lesson, we\'ll cover some testing best practices \u2014 as well as go into a deeper dive regarding the differences between good and bad fails. Writing good fails instead of bad ones can really trip students up at first. And the implications of bad fails can be significant \u2014 extra time trying to find bugs, frustration, and less understanding about what\'s going wrong in the code. The implications get even bigger once we are out in the real world \u2014 for instance, if we don\'t test our code correctly, we might introduce breaking changes to production code, making our customers, coworkers, and employers upset. If possible, we don\'t want to do that.","source":"@site/docs/2_intermediate-javascript/2_test-driven-development-and-environments-with-javascript/2-2-1-08-testing-best-practices.md","sourceDirName":"2_intermediate-javascript/2_test-driven-development-and-environments-with-javascript","slug":"/intermediate-javascript/test-driven-development-and-environments-with-javascript/2-2-1-8-testing-best-practices","permalink":"/v1.2/intermediate-javascript/test-driven-development-and-environments-with-javascript/2-2-1-8-testing-best-practices","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"title":"\ud83d\udcd3 2.2.1.8 Testing Best Practices","day":"monday","id":"2-2-1-8-testing-best-practices","hide_table_of_contents":true},"sidebar":"intermediate-javascript","previous":{"title":"\ud83d\udcd3 2.2.1.7 TDD with Jest: Testing the Triangle.prototype.checkType() Method","permalink":"/v1.2/intermediate-javascript/test-driven-development-and-environments-with-javascript/2-2-1-7-tdd-with-jest-testing-the-triangle-prototype-checktype-method"},"next":{"title":"\ud83d\udcd3 2.2.1.9 Expanding our Testing Tools: Adding Setup and Teardown","permalink":"/v1.2/intermediate-javascript/test-driven-development-and-environments-with-javascript/2-2-1-9-expanding-our-testing-tools-adding-setup-and-teardown"}}');var a=n(74848),i=n(28453);const o={title:"\ud83d\udcd3 2.2.1.8 Testing Best Practices",day:"monday",id:"2-2-1-8-testing-best-practices",hide_table_of_contents:!0},r=void 0,d={},c=[{value:"Good Testing Practices \u2014 and Failing Right",id:"good-testing-practices--and-failing-right",level:2},{value:"Bad Fails",id:"bad-fails",level:2},{value:"Summary",id:"summary",level:2}];function l(e){const t={code:"code",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"In this lesson, we'll cover some testing best practices \u2014 as well as go into a deeper dive regarding the differences between good and bad fails. Writing good fails instead of bad ones can really trip students up at first. And the implications of bad fails can be significant \u2014 extra time trying to find bugs, frustration, and less understanding about what's going wrong in the code. The implications get even bigger once we are out in the real world \u2014 for instance, if we don't test our code correctly, we might introduce breaking changes to production code, making our customers, coworkers, and employers upset. If possible, we don't want to do that."}),"\n",(0,a.jsx)(t.p,{children:"Unfortunately, in the real world, there are many companies that don't test their code, though it's not due to anyone arguing that untested code is better. Instead, tight deadlines, small staff, tight budgets, poor management practices, legacy code, and many other complications can lead to code that isn't fully tested, or tested at all."}),"\n",(0,a.jsx)(t.p,{children:"At Epicodus, we're focused on teaching best practices for testing. Ideally, you'll eventually work at a company that has good testing practices. Even if the company doesn't have good practices, you can bring your newfound knowledge to the company and make a difference immediately. In fact, it's very common for junior developers to start learning a codebase by writing tests first. Adding testing to a codebase is something you can often start doing right away \u2014 even if you don't fully understand the code you're working with. And doing so can add tremendous value to your company."}),"\n",(0,a.jsx)(t.h2,{id:"good-testing-practices--and-failing-right",children:"Good Testing Practices \u2014 and Failing Right"}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:"Here are some testing best practices \u2014 as well as steps to make sure you get good fails \u2014 and in turn, good passes."}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Write tests for distinct behaviors."})," Often a function just needs one test, because it does one thing. However, if our function does multiple things, we should consider having a test for each behavior. This improves how easy to understand our code and tests are. It's OK to have multiple expectations for a single test. However, if a test has many expectations it can be a red flag that the test may cover multiple distinct behaviors and may need to be broken into multiple tests to improve comprehension."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsxs)(t.strong,{children:["Use ",(0,a.jsx)(t.code,{children:"Describe"})," blocks to improve comprehension of your tests."]})," You can use ",(0,a.jsx)(t.code,{children:"Describe"})," blocks for entire object types or for individual methods and functions, depending on how complicated each is. Similar to writing tests for distinct behaviors to clearly communicate the functionality of your code, ",(0,a.jsx)(t.code,{children:"Describe"})," blocks are meant to improve organization and comprehension of your test. If a class is more complex, you may want to use a ",(0,a.jsx)(t.code,{children:"Describe"})," block for each method instead of just the class."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Write the test first, not the code."})," We've already covered many of the reasons why we should write our tests before we write any code. This is the cornerstone of TDD because it's ",(0,a.jsx)(t.em,{children:"test"}),"-driven, not ",(0,a.jsx)(t.em,{children:"code"}),"-driven. We won't reiterate all the reasons we should write tests first as we've covered that elsewhere."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Write just enough of a function or method to get the code to fail."})," We can't just write a test, run it, and move on. That would be a bad fail. If we are testing a function, for instance, we need to at least add the ",(0,a.jsx)(t.code,{children:"function"})," keyword and the name of the function. Our test should at least return how our expectation wasn't met \u2014 such as by stating the expected result and returning the actual result (such as ",(0,a.jsx)(t.code,{children:"undefined"})," because we haven't written a function body yet)."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Keep the code in your test to a minimum."})," In your tests, you should only write code that's required to run the piece of business logic that you are testing for. This usually involves invoking the business logic function you are testing and not much more. Adding more code than that can create problems by introducing bugs that are unrelated to our business logic. We want to isolate problems in our code, not create more problems by adding unnecessary code in our tests."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Read the Jest output for failing tests."})," It's easy to just run a test, see that it's red, and assume that it's a good fail. It's tempting to be in a rush to write the code, especially if we're excited about it or have an idea about how to implement it. But just because it's red doesn't mean it's a good fail."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Always fix bad fails before moving onto the code."})," TDD means having a good fail before writing the code to pass the test. It doesn't mean writing a test, having a bad fail, and then writing some code. That is actually a recipe for disaster. We'll often see students looking for bugs in the wrong places when this happens \u2014 or just being utterly confused because their tests aren't pointing them in the right direction."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Always commit your code after each passing test."})," This is part of having a strong commit history. Also, if you break your code and can't get it working again, you can always return to a commit where all tests are passing."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Next, let's look at some examples of bad fails."}),"\n",(0,a.jsx)(t.h2,{id:"bad-fails",children:"Bad Fails"}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:"Let's cover some examples of bad fails based on the following tests:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:"import Example from '../src/example.js';\n\ndescribe('Example', () => {\n\n  test('should correctly demonstrate bad fails', () => {\n    let example = new Example();\n    expect(example.data).toEqual(\"Bad fail!\");\n  });\n\n  test('should correctly demonstrate bad fails', () => {\n    let example = new Example();\n    expect(example.exampleFunction()).toEqual(\"This function returns a bad fail!\");\n  });\n});\n"})}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The test won't run because it can't find a file."})," Let's say that when we run the test suite above, we get the following error: ",(0,a.jsx)(t.code,{children:"Cannot find module '../src/example.js' from 'example.test.js'"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["This means that the file doesn't exist ",(0,a.jsx)(t.em,{children:"or"})," there's an error in the path ",(0,a.jsx)(t.em,{children:"or"})," the file exists but there is an error in the file name."]}),"\n",(0,a.jsx)(t.p,{children:"This is a bad fail. Our test should always be able to correctly find the file it is looking for. We're not even correctly testing any code yet. All we've confirmed is that our tests can't find a file."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The test won't run because it can't find a function or constructor."})," So we realize that we don't have a ",(0,a.jsx)(t.code,{children:"src/example.js"})," file yet \u2014 or that it's named ",(0,a.jsx)(t.code,{children:"src/exmple.js"})," \u2014 or even that for some reason it's not in the ",(0,a.jsx)(t.code,{children:"src"})," directory. We fix that issue and run the test, only to get the following error:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:"TypeError: _example.default is not a constructor\n"})}),"\n",(0,a.jsx)(t.p,{children:"This means that one of three things could be happening:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["We haven't added a constructor for ",(0,a.jsx)(t.code,{children:"Example"})," yet."]}),"\n",(0,a.jsxs)(t.li,{children:["There's a typo either in our test (such as ",(0,a.jsx)(t.code,{children:"new Exmple()"}),")."]}),"\n",(0,a.jsx)(t.li,{children:"There's a typo in our constructor."}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"In the case of a missing function, we'll get an error like this:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:"TypeError: example.exampleFunction is not a function\n"})}),"\n",(0,a.jsx)(t.p,{children:"These are both bad fails. Any functions or constructors that a test uses at the very least need to exist so that the test can properly run or test the constructor."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The test won't run because there's an error that breaks all the tests in a suite."})," An error in a test file can blow up the whole suite \u2014 or it can cause an individual test to fail."]}),"\n",(0,a.jsx)(t.p,{children:"Here's an example that will blow up the entire suite:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:"Test suite failed to run\n\n    SyntaxError: /Users/staff/Desktop/test_env/__tests__/example.test.js: Unexpected token, expected \";\" (1:6)\n\n    > 1 | impor Example from '../src/example.js';\n        |       ^\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Can you see the error? There's a typo in the import statement \u2014 it should be ",(0,a.jsx)(t.code,{children:"import"}),", not ",(0,a.jsx)(t.code,{children:"impor"}),". Any time you see ",(0,a.jsx)(t.code,{children:"Test suite failed to run"})," for any reason, it's a bad fail. It means the entire test file isn't able to run. Don't you dare start writing your code yet if you're in this situation! Get the tests working first."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The test won't run because there's an error in an individual test."})," An error can also break just one test as well. Here's an example:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:"should correctly demonstrate bad fails\n\n    ReferenceError: example is not defined\n\n       9 | \n      10 |   test('should correctly demonstrate bad fails', () => {\n    > 11 |     expect(example.exampleFunction()).toEqual(\"This function returns a bad fail!\");\n         |            ^\n      12 |   });\n"})}),"\n",(0,a.jsxs)(t.p,{children:["Here we get the error ",(0,a.jsx)(t.code,{children:"ReferenceError: example is not defined"}),". In this case, it can be tempting to think this error is happening because we haven't defined something in our source code. Take a closer look at the error, though. In reality, it's because the line ",(0,a.jsx)(t.code,{children:"let example = new Example();"})," has been removed from the test \u2014 and there's no variable named ",(0,a.jsx)(t.code,{children:"example"})," in our test's scope."]}),"\n",(0,a.jsx)(t.p,{children:"There are a million and one different errors that can occur in a test \u2014 but all of them are bad fails. The test needs to be running correctly and be error-free or it will be a bad fail. Once again, make sure you get this working correctly before writing any code."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The test fails because we wrote bogus data in our test."})," Let's say we've got all the code we need in our constructor to get the first test passing:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:'export default function Example() {\n  this.data = "Bad fail!";\n}\n'})}),"\n",(0,a.jsx)(t.p,{children:"However, for some reason we want to make sure that we get a fail before our test passes... perhaps because we did things out of order and wrote our code before we wrote the test. So we update our test to do the following:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:"  test('should correctly demonstrate bad fails', () => {\n    let example = new Example();\n    expect(example.data).toEqual(\"I'm sneaky! I'm gonna pretend this is a good fail!\");\n  });\n"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:'expect(received).toEqual(expected) // deep equality\n\nExpected: "I\'m sneaky! I\'m gonna pretend this is a good fail!"\nReceived: "Bad fail!"\n'})}),"\n",(0,a.jsx)(t.p,{children:"This is what is known as a test-based fail, not a code-based fail. The problem here is that we aren't actually testing to see if our code is working correctly \u2014 if we were, our expect statement would state"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:'expect(example.data).toEqual("Bad fail!");\n'})}),"\n",(0,a.jsxs)(t.p,{children:["With ",(0,a.jsx)(t.code,{children:'"Bad fail!"'})," to match the code in the constructor."]}),"\n",(0,a.jsxs)(t.p,{children:["However, by changing the expect statement to match with ",(0,a.jsx)(t.code,{children:"\"I'm sneaky! I'm gonna pretend this is a good fail!\""}),", we're trying to manufacture a fail, instead of letting one happen naturally."]}),"\n",(0,a.jsx)(t.p,{children:"The tests we write always need to be a source of truth. We always need to write a test so that it passes. If we change the code in our test to fail it, we could start getting confused, and we might run into false positives (or negatives). In other words, if we've inserted errors into the test, it's no longer a source of truth, and it's no longer reliable for testing our code."}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"The test fails (or even passes) because we put too much code in our test."})," Let's say we want to test that our constructor automatically adds a ",(0,a.jsx)(t.code,{children:"data"})," property of ",(0,a.jsx)(t.code,{children:'"Bad fail!"'})," to an instance of ",(0,a.jsx)(t.code,{children:"Example"}),". This would be a truly horrible test:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:'// This is hideous.\n\n  test(\'should instantiate an Example with a data property of Bad fail!\', () => {\n    let example = new Example();\n    const data1 = "Bad "\n    const data2 = "fail"\n    const data3 = "!"\n    const newData = data1.concat(data2).concat(data3);\n    example.data = newData;\n    expect(example.data).toEqual("Bad fail!");\n  });\n'})}),"\n",(0,a.jsxs)(t.p,{children:["This test is actually going to pass if we at least correctly instantiate an ",(0,a.jsx)(t.code,{children:"Example"})," object. But it's such a bad pass that it's truly a bad fail. What happens here is that we've written code in the test to get the test to artificially pass. But we're supposed to be testing our source code, and we're not actually testing that when an ",(0,a.jsx)(t.code,{children:"Example"})," is instantiated, a ",(0,a.jsx)(t.code,{children:"data"})," property with the right value is being created. Instead, we're writing the code manually in the test."]}),"\n",(0,a.jsx)(t.p,{children:"If we were quality control elves in Santa's lab and our job was to inspect little toy cars being churned out of a machine to make sure they are painted red, we can't just take a car that's painted blue, paint it red, and say the machine is working correctly. In this analogy, our code is the machine. Always test to make sure the machine works. Don't alter the results (the test) to fit what you're hoping for."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.strong,{children:"And here's the worst fail of all:"})}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{children:"Test suite failed to run\n\nYour test suite must contain at least one test.\n"})}),"\n",(0,a.jsx)(t.p,{children:"This means your suite doesn't even have any tests in it yet. Time to start adding some tests!"}),"\n",(0,a.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(t.hr,{}),"\n",(0,a.jsx)(t.p,{children:"Before we move on, let's reiterate an example of a good fail. We can do the following to ensure a good fail:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Make sure the file is being accessed (no test errors)."}),"\n",(0,a.jsx)(t.li,{children:"Make sure the function is being called."}),"\n",(0,a.jsxs)(t.li,{children:["It's fine if the function returns ",(0,a.jsx)(t.code,{children:"undefined"})," \u2014 don't write bogus code in either the function or your test to turn a good fail into a bad one."]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Here's the error message on a good fail:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-js",children:'expect(received).toEqual(expected) // deep equality\n\nExpected: "This function returns a good fail!"\nReceived: undefined\n'})}),"\n",(0,a.jsxs)(t.p,{children:["Here, we can see that the function was correctly called, which implies that our test was able to find the file as well. The function returns ",(0,a.jsx)(t.code,{children:"undefined"})," because we haven't added any code to the function body yet."]}),"\n",(0,a.jsx)(t.p,{children:"At this point, you should have a clear sense of the difference between good and bad fails. Ensuring your test has a good fail first is an important part of the process, and doing so can help you isolate bugs in your code. In general, these testing practices apply no matter what language you are writing in \u2014 with a few small modifications depending on the language."}),"\n",(0,a.jsx)(t.p,{children:"From here on out, you should be applying these best practices as best as you can whether you are writing tests in JavaScript, Ruby, C#, or another language."})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}}}]);